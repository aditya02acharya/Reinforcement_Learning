{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook for Deep Q-Learning Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Method is a Model-Free, Value-based and Off-policy.\n",
    "\n",
    "- It doesn't build any model of the environment.\n",
    "- It approximates the policy indirectly by finding the value of taking an action.\n",
    "- It train on historic data obtained from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps:\n",
    "\n",
    "1. Initialise parameters for $Q_{policynet}(s,a)$ and $Q_{targetnet}(s,a)$ with random weights, $\\epsilon = 1.0$, and empty replay buffer.\n",
    "2. With probability $\\epsilon$, select a random action $a$, otherwise $argmax_a Q_{policynet}(s,a)$.\n",
    "3. Execute action $a$ and observe the reward $r$ and next state $s'$.\n",
    "4. Store the transion $<s, a, r, s'>$ in the replay buffer.\n",
    "5. Sample a random minibatch of transitions from the replay buffer.\n",
    "6. For every transition in the buffer, calculate target $y = r$, if it is the terminal state or $y = r + \\gamma max_{a'} Q_{targetnet}(s', a')$ otherwise.\n",
    "7. Calculate loss $L = (Q_{policynet}(s,a) - y)^2$\n",
    "8. Update Q(s,a) using Stocastic Gradient Decent algorithm by minimising the loss.\n",
    "9. After every N steps copy parameters from $Q_{policynet}$ to $Q_{targetnet}$\n",
    "10. Repeat from step 2 until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utility import env_wrappers\n",
    "from Agents.dqn_agent import DQN\n",
    "\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent-Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env_wrappers.make_env(\"PongNoFrameskip-v4\")\n",
    "\n",
    "obs_size = environment.observation_space.shape[0]\n",
    "n_actions = environment.action_space.n\n",
    "\n",
    "policy_net = Net(environment.observation_space.shape, environment.action_space.n)\n",
    "target_net = Net(environment.observation_space.shape, environment.action_space.n)\n",
    "\n",
    "agent = DQN(env=environment, network_policy=policy_net, network_target=target_net, memory_size=10000, \n",
    "            learning_rate=1e-4, target_perf=19.5, replay_warmup=10000, target_update=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env_wrappers.make_env(\"PongNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
