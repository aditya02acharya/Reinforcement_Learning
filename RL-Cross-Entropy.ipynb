{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook for Cross-Entorpy Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Method is a *Model-Free*, *Policy-based* and *On-policy*.\n",
    "\n",
    "- It doesn't build any model of the environment.\n",
    "- It approximates the policy of the agent.\n",
    "- It train on fresh data obtained from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Steps:\n",
    "\n",
    "1. Play N number of Episodes, using current model.\n",
    "2. Calculate the total reward for every episode and decide on a reward cut-off boundary. \n",
    "3. Throw away all episodes with total reward below the boundary.\n",
    "4. Train on the remaining episodes using observations as the input and issued actions as desired output.\n",
    "5. Repeat from step 1. untill total reward per epsiode converges or until desired performance is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from Agents.cross_entropy_agent import Cross_Entropy_Agent\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "CUT_OFF_PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent-Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"CartPole-v0\")\n",
    "\n",
    "obs_size = environment.observation_space.shape[0]\n",
    "n_actions = environment.action_space.n\n",
    "\n",
    "network = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "agent = Cross_Entropy_Agent(environment=environment, network=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.685, reward_mean=20.8, reward_bound=20.0\n",
      "1: loss=0.670, reward_mean=22.3, reward_bound=23.0\n",
      "2: loss=0.673, reward_mean=19.1, reward_bound=22.5\n",
      "3: loss=0.688, reward_mean=21.1, reward_bound=27.5\n",
      "4: loss=0.684, reward_mean=25.2, reward_bound=27.0\n",
      "5: loss=0.669, reward_mean=30.9, reward_bound=38.5\n",
      "6: loss=0.681, reward_mean=37.4, reward_bound=44.5\n",
      "7: loss=0.665, reward_mean=26.7, reward_bound=30.0\n",
      "8: loss=0.652, reward_mean=40.0, reward_bound=45.0\n",
      "9: loss=0.625, reward_mean=32.8, reward_bound=35.0\n",
      "10: loss=0.641, reward_mean=44.0, reward_bound=46.0\n",
      "11: loss=0.620, reward_mean=31.6, reward_bound=35.5\n",
      "12: loss=0.634, reward_mean=41.5, reward_bound=47.0\n",
      "13: loss=0.597, reward_mean=42.9, reward_bound=46.0\n",
      "14: loss=0.609, reward_mean=54.1, reward_bound=66.0\n",
      "15: loss=0.609, reward_mean=45.3, reward_bound=57.5\n",
      "16: loss=0.615, reward_mean=53.6, reward_bound=58.0\n",
      "17: loss=0.596, reward_mean=55.8, reward_bound=76.0\n",
      "18: loss=0.594, reward_mean=60.4, reward_bound=64.0\n",
      "19: loss=0.595, reward_mean=59.9, reward_bound=61.5\n",
      "20: loss=0.584, reward_mean=63.9, reward_bound=77.0\n",
      "21: loss=0.595, reward_mean=57.1, reward_bound=60.0\n",
      "22: loss=0.585, reward_mean=66.6, reward_bound=65.5\n",
      "23: loss=0.572, reward_mean=77.4, reward_bound=94.5\n",
      "24: loss=0.566, reward_mean=61.3, reward_bound=68.0\n",
      "25: loss=0.561, reward_mean=83.0, reward_bound=91.0\n",
      "26: loss=0.582, reward_mean=97.2, reward_bound=124.5\n",
      "27: loss=0.555, reward_mean=82.6, reward_bound=93.0\n",
      "28: loss=0.569, reward_mean=107.6, reward_bound=136.5\n",
      "29: loss=0.558, reward_mean=118.9, reward_bound=139.5\n",
      "30: loss=0.551, reward_mean=126.1, reward_bound=148.5\n",
      "31: loss=0.541, reward_mean=103.7, reward_bound=109.0\n",
      "32: loss=0.530, reward_mean=119.1, reward_bound=128.5\n",
      "33: loss=0.542, reward_mean=113.8, reward_bound=151.0\n",
      "34: loss=0.540, reward_mean=144.5, reward_bound=181.5\n",
      "35: loss=0.544, reward_mean=131.9, reward_bound=158.5\n",
      "36: loss=0.536, reward_mean=154.9, reward_bound=172.5\n",
      "37: loss=0.537, reward_mean=151.8, reward_bound=170.0\n",
      "38: loss=0.535, reward_mean=173.8, reward_bound=199.0\n",
      "39: loss=0.537, reward_mean=166.4, reward_bound=200.0\n",
      "40: loss=0.527, reward_mean=174.9, reward_bound=200.0\n",
      "41: loss=0.527, reward_mean=184.0, reward_bound=200.0\n",
      "42: loss=0.531, reward_mean=182.3, reward_bound=200.0\n",
      "43: loss=0.516, reward_mean=176.8, reward_bound=200.0\n",
      "44: loss=0.523, reward_mean=190.0, reward_bound=200.0\n",
      "45: loss=0.514, reward_mean=195.0, reward_bound=200.0\n",
      "46: loss=0.519, reward_mean=199.9, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "agent.run(BATCH_SIZE, CUT_OFF_PERCENTILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
